{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f81fe7",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024397e",
   "metadata": {},
   "source": [
    "### 1. 信息论的函数为什么定义为$H(p)=-\\sum_{i=1}^np_i\\log p_i$的形式？定义成这样的形式有什么好处？\n",
    "\n",
    "Shannon于1948年，在创造信息论的著名论文《A Mathematical Theory of Communication》中指出如下定理：\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>定理1</h4><p>\n",
    "如果度量不确定性的函数$H(p)$满足以下三个条件：\n",
    "    \n",
    "* $H$关于$p_i$是连续函数<br/><br/>\n",
    "* 若$p_1=p_2=\\cdots=p_n$，则$H(p)$关于$n$单调递增<br/><br/>\n",
    "* 若将一个 $p_i$拆分成 $p_{i,1}$和 $p_{i,2}$，则\n",
    "    \n",
    "    $$H(p_1,\\cdots,p_{i-1},p_{i,1},p_{i,2},p_{i+1},\\cdots,p_n)=H(p_1,\\cdots,p_{i-1},p_{i},p_{i+1},\\cdots,p_n)+\n",
    "    p_iH(\\frac{p_{i,1}}{p_i},\\frac{p_{i,2}}{p_i})$$\n",
    "    \n",
    "那么$H$的形式只能是\n",
    "    \n",
    "$$H(p_1,p_2,\\cdots,p_n)= -C\\sum_{i=1}^np_i\\log p_i$$\n",
    "    \n",
    "</p></div>\n",
    "\n",
    "其中，第二个条件直观地体现了$H(p)$会随着概率的分散而增加。\n",
    "\n",
    "条件三说明将某个事件拆分为多个事件时的不确定性增加了，并且增加的不确定性与拆分时的比例和拆分事件的概率有关。拆分高概率事件比拆分低概率事件增加的混乱程度更大。拆分概率拆得越均匀，不确定性越大。\n",
    "\n",
    "支撑集为$K$个不同实数的离散分布，**熵在单点分布时最小，此时熵为0；在均匀分布时最大，此时熵为$\\log K$**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74944c",
   "metadata": {},
   "source": [
    "### 2. 为什么要研究条件熵？条件熵应该怎么计算？它有哪些性质？\n",
    "\n",
    "在决策树的分裂过程中，我们不但需要考察某个节点的不确定性或纯度，而且还要考察它的子节点的平均不确定性或平均纯度来决定是否进行分裂。子节点的产生来源于决策树分支的条件，因此我们不但要研究随机变量的信息熵，还要研究在给定条件下随机变量的平均信息熵或条件熵。\n",
    "\n",
    "条件熵的计算公式为\n",
    "\n",
    "$$H(Y|X)=E_X[E_{Y|X}[-\\log_2P(Y|X)]]$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><h4>定理2</h4><p>\n",
    "    设$Y$和$X$是离散随机变量，$Y$关于$X$的信息增益非负。\n",
    "</p></div>\n",
    "    \n",
    "根据条件熵可以定义信息增益\n",
    "\n",
    "$$G(Y,X)=H(Y)-H(Y|X)$$\n",
    "\n",
    "本质上，信息增益可以理解为$p(y,x)$关于$p(y)p(x)$的KL散度，\n",
    "\n",
    "$$G(Y,X)=-\\sum_{k=1}^K\\sum_{m=1}^Mp(y_k,x_m)\\log_2\\frac{p(y_k)p(x_m)}{p(y_k,x_m)}$$\n",
    "\n",
    "因此我们立即可以看出，**信息增益是对称的**，交换$x$与$y$并不会改变计算结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd0fe90",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习1</h4><p>\n",
    "    证明以下关系：\n",
    "\n",
    "* $G(Y,X)=H(X)-H(X|Y)$<br/><br/>\n",
    "* $G(Y,X)=H(X)+H(Y)-H(Y,X)$<br/><br/>\n",
    "* $G(Y,X)=H(Y,X)-H(X|Y)-H(Y|X)$\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5999a",
   "metadata": {},
   "source": [
    "* 第一条结论由信息增益的对称性可以证明<br/><br/>\n",
    "* 第二条结论：\n",
    "\\begin{align*}\n",
    "H(X)+H(Y)-G(Y,X) & = -\\sum_{m=1}^Mp(x_m)\\log_2p(x_m)-\\sum_{k=1}^Kp(y_k)\\log_2p(y_k)+\\sum_{m=1}^M\\sum_{k=1}^Kp(x_m,y_k)\\log_2\\frac{p(x_m)p(y_k)}{p(x_m,y_k)} \\\\\n",
    "& = -\\sum_{m=1}^M\\sum_{k=1}^Kp(x_m,y_k)\\log_2p(x_m)-\\sum_{m=1}^M\\sum_{k=1}^Kp(x_m,y_k)\\log_2p(y_k)+\\sum_{m=1}^M\\sum_{k=1}^Kp(x_m,y_k)\\log_2\\frac{p(x_m)p(y_k)}{p(x_m,y_k)} \\\\\n",
    "& = -\\sum_{m=1}^M\\sum_{k=1}^Kp(x_m,y_k)\\log_2p(x_m,y_k) = H(Y,X)\n",
    "\\end{align*}\n",
    "* 第三条结论：\n",
    "\n",
    "$$G(Y,X)=H(Y)-H(Y|X)\\tag{1}$$\n",
    "\n",
    "$$G(Y,X)=H(X)-H(X|Y)\\tag{2}$$\n",
    "\n",
    "$$G(Y,X)=H(X)+H(Y)-H(Y,X)\\tag{3}$$\n",
    "\n",
    "$(1)+(2)-(3)$即可得到结论"
   ]
  },
  {
   "attachments": {
    "graph1.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4cdf444c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习2</h4><p>\n",
    "    下图区域被分成了A、B、C三个区域，若AB区域代表X的不确定性，BC区域代表Y的不确定性，那么$H(X), H(Y), H(X|Y), H(Y|X), H(Y,X)$和$G(Y,X)$分别指代的是哪篇区域？\n",
    "    \n",
    "![graph1.PNG](attachment:graph1.PNG)\n",
    "</p></div>\n",
    "\n",
    "$H(X)$指代的是$A\\cup B$，$H(Y)$指代的是$B\\cup C$，$H(X|Y)$指代的是$A$，$H(Y|X)$指代的是$C$，H(Y,X)指代的是$A\\cup B\\cup C$，$G(Y,X)$指代的是$B$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35084784",
   "metadata": {},
   "source": [
    "### 3. 三种决策树是怎么分类的？有哪些不同点？\n",
    "\n",
    "|决策树类型|类别特征|数值特征|含缺失值的特征|优势|\n",
    "|:---:|:---|:---|:---|:---|\n",
    "|ID3|目标：每个节点选择最大化**信息增益**分裂|无法处理/转化为类别特征|无法处理|无|\n",
    "|C4.5|目标：每个节点选择最大化**信息增益比**分类|通过最佳分割或均匀分割<br>将数值特征转化为类别特征处理|对含缺失值的特征进行信息增益比惩罚<br>缺失值占比越大惩罚越大|(1).削弱了ID3过度分裂偏好<br>(2).能够处理数值特征和缺失值特征<br>(3).给出了剪枝策略|\n",
    "|CART|无法处理/转化为二分类|目标：每个节点选择最大化基于<br>**基尼系数的信息增益**分类|由用户自行决定|(1).既能分类又能回归<br>回归树输出叶节点下所有标签的均值<br>(2).改用基尼系数，简化熵的计算|\n",
    "\n",
    "一些概念：\n",
    "* C4.5-信息增益比\n",
    "$$G^R(Y,X)=\\frac{G(Y,X)}{H(X)}$$\n",
    "\n",
    "* CART-回归信息增益\n",
    "$$G^{\\text{MSE}}(Y,X)=\\text{MSE}-\\Big(\\frac{N_L}{N}\\text{MSE}_L+\\frac{N_R}{N}\\text{MSE}_R\\Big)$$\n",
    "\n",
    "* CART-分类基尼系数\n",
    "$$\\text{Gini}(Y)=E_Y[1-p(Y)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaae69",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习3</h4><p>\n",
    "    假设当前我们需要处理一个分类问题，请问对输入特征进行归一化会对树模型的类别输出产生影响吗？请解释原因。\n",
    "</p></div>\n",
    "\n",
    "首先，**特征归一化并不会对结果造成影响**，因为归一化属于单调的线性变换，而数值缩放并不会影响分裂点的位置。只要排序的顺序不变，那么所属的分支以及分裂点就不会产生区别。\n",
    "\n",
    "但是，归一化的好处是可以加快梯度下降求解的速度，而**树模型是不能进行梯度下降**的，因为构建树模型寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，求导也没有意义，因此**在实际运行中不需要归一化**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7248de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习4</h4><p>\n",
    "    在处理有缺失值得特征时，我们使用修正的信息增益\n",
    "    \n",
    "$$\\tilde{G}(Y,X)=(1-\\gamma)G(\\tilde{Y},\\tilde{X})$$\n",
    "    \n",
    "其中$\\tilde{Y},\\tilde{X}$是非缺失值的标签和特征，$\\gamma$是缺失值比例。如果将系数替换为$1-\\gamma^2$，请问对缺失值是加强了还是削弱了惩罚？\n",
    "</p></div>\n",
    "\n",
    "我们知道$0\\leqslant\\gamma\\leqslant1$，因此$0\\leqslant\\gamma^2\\leqslant\\gamma\\leqslant 1$，因此$G(\\tilde{Y},\\tilde{X})$前面的系数变大了，对缺失值的惩罚也就削弱了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951723b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习5</h4><p>\n",
    "    如果将树的生长策略从深度优先生长改为广度优先生长，假设其他参数保持不变的情况下，两个模型对应的结果输出可能不同吗？\n",
    "</p></div>\n",
    "\n",
    "不管是深度优先增长还是广度优先增长，每个节点都是达到停止条件后才停止分裂，各个叶节点之间的分裂互不干扰，其区别只是搜索顺序的区别，因此两个模型对应的结果输出是相同的。\n",
    "\n",
    "但是，深度优先生长和最佳增益生长对应的输出是完全可能不相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b2a29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习6</h4><p>\n",
    "    在一般的机器学习问题中，我们总是通过一组参数来定义模型的损失函数，并且在训练集上以最小化该损失函数为目标进行优化。请问对于决策树而言，模型优化的目标是什么？\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a723f29",
   "metadata": {},
   "source": [
    "对于决策树而言，优化目标是最大化信息增益/信息增益比/基尼系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a235af8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习7</h4><p>\n",
    "    对信息熵中的$log$函数在$p=1$处进行一阶泰勒展开可以近似为基尼系数，那么如果在$p=1$处进行二阶泰勒展开我们可以获得什么近似指标？请写出对应指标的信息增益公式。\n",
    "</p></div>\n",
    "\n",
    "对应的信息增益为\n",
    "\\begin{align*}\n",
    "H(Y) = E_YI(p)=E_Y[-\\log_2p(Y)] & \\approx E_Y[1-p(Y)+\\frac{1}{2}(1-p(Y))^2] \\\\\n",
    "& = \\sum_{k=1}^K\\tilde{p}(y_k)(1-\\tilde{p}(y_k)+\\frac{1}{2}(1-\\tilde{p}(y_k))^2) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(Y|X) = E_X[E_{Y|X}[-\\log_2p(Y)]] & \\approx E_X[E_{Y|X}[1-p(Y)+\\frac{1}{2}(1-p(Y))^2]] \\\\\n",
    "& = \\sum_{m=1}^M\\tilde{p}(x_m)\\sum_{k=1}^K\\tilde{p}(y_k|x_m)(1-\\tilde{p}(y_k|x_m)+\\frac{1}{2}(1-\\tilde{p}(y_k|x_m))^2)\n",
    "\\end{align*}\n",
    "\n",
    "$$G(Y,X)=H(Y)-H(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d820e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习8</h4><p>\n",
    "    除了信息熵和基尼系数之外，我们还可以使用节点的$1-\\max_kp(Y=y_k)$和第$m$个子节点的$1-\\max_kp(Y=y_k|X=x_m)$来作为衡量纯度的指标。请解释其合理性并给出相应的信息增益公式。\n",
    "</p></div>\n",
    "\n",
    "这种衡量混乱程度的指标$F(p)=1-\\max_i{p_i}$满足以下的性质：\n",
    "\n",
    "* $F(p)$对$p$是连续非负有界的，当且仅当随机变量以概率1为常数时有$F(p)=0$<br><br>\n",
    "* 如果$p_1=p_2=\\cdots=p_n$，那么$F(p)$关于$n$单调递增\n",
    "\n",
    "信息增益公式为\n",
    "\n",
    "\\begin{align*}\n",
    "G(Y,X) & = F(Y)-F(Y|X) \\\\\n",
    "& = 1-\\max_kp(Y=y_k)-\\sum_{m=1}^Mp(X=x_m)[1-\\max_kp(Y=y_k|X=x_m)] \\\\\n",
    "& = \\sum_{m=1}^Mp(X=x_m)\\max_kp(Y=y_k|X=x_m)-\\max_kp(Y=y_k)\n",
    "\\end{align*}\n",
    "\n",
    "根据信息增益公式，我们可以推理出第三条性质：\n",
    "\n",
    "* 信息增益$G(Y,X)\\geqslant 0$，且$Y$与$X$独立时$G(Y,X)=0$\n",
    "\n",
    "证明：不失一般性，假设$Y=y_1$时概率最大，则\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_kp(Y=y_k) = p(Y=y_1) & = \\sum_{m=1}^Mp(X=x_m)p(Y=y_1|X=x_m) \\\\\n",
    "& \\leqslant \\sum_{m=1}^Mp(X=x_m)\\max_kp(Y=y_k|X=x_m)\n",
    "\\end{align*}\n",
    "\n",
    "不难得到$G(Y,X)\\geqslant 0$，且$Y$与$X$独立时等号成立(但等号成立不能保证独立)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01997a6",
   "metadata": {},
   "source": [
    "### 4. 决策树的剪枝策略是怎样的？\n",
    "\n",
    "决策树的剪枝策略主要有两种：**预剪枝**和**后剪枝**。\n",
    "\n",
    "* 预剪枝是指树在判断节点是否分裂的时候就预先通过一些规则来阻止其分裂；\n",
    "\n",
    "预剪枝通过确定模型的最大树深度、节点分裂的最小样本数等参数进行控制。\n",
    "\n",
    "\n",
    "* 后剪枝是指在树的节点已经全部生长完成后，通过一些规则来摘除一些子树。\n",
    "\n",
    "后剪枝定义了树的剪枝度量$T$，\n",
    "\n",
    "$$R_\\alpha(T^N)=R(T^N)+\\alpha|T^N|$$\n",
    "\n",
    "其中$R(T^N)$表示不纯度，$\\alpha$是参数，$|T^N|$是叶节点数量。\n",
    "\n",
    "其基本思想是，如果对于决策树某一个节点为根的子树，其根的剪枝度量低于该子树的剪枝度量，那么这个根节点就没有必要分裂。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04cf8f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习9</h4><p>\n",
    "    为什么对没有重复特征值的数据，决策树能够做到损失为0？\n",
    "</p></div>\n",
    "\n",
    "在没有重复特征值数据的情况下，决策树总是可以不断分裂，使得每个叶节点只包含一个实例，使得损失为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60868005",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>练习10</h4><p>\n",
    "    如何理解min_samples_leaf参数能够控制回归树输出值的平滑程度？\n",
    "</p></div>\n",
    "\n",
    "因为叶节点样本数越小，回归取值相对越多，间断点越多，且各叶节点取值的均方误差越大，因此越不光滑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39f5e3",
   "metadata": {},
   "source": [
    "### 知识回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2cef2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>知识回顾1</h4><p>\n",
    "    ID3树算法、C4.5树算法和CART算法之间有何异同？\n",
    "</p></div>\n",
    "\n",
    "相同点：\n",
    "\n",
    "* 都属于决策树算法，都可以有效实现分类任务。\n",
    "\n",
    "* 都通过最大化信息增益/信息增益比进行分裂。\n",
    "\n",
    "不同点可见上文整理部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6176c0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>知识回顾2</h4><p>\n",
    "    什么是信息增益？它衡量了什么指标？它有什么缺陷？\n",
    "</p></div>\n",
    "\n",
    "信息增益是指得到了随机变量X的取值信息时，随机变量Y不确定性的平均减少量。即节点分裂之后带来了多少不确定性的降低或纯度的提高。\n",
    "\n",
    "信息增益对类别较多的特征存在偏好，使用了信息增益比来代替信息增益更合适。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed64e09",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>知识回顾3</h4><p>\n",
    "    sklearn决策树中的random_state参数控制了哪些步骤的随机性？\n",
    "</p></div>\n",
    "\n",
    "random_state参数可以控制抽出max_features个特征的随机性。\n",
    "\n",
    "如果设置random_state为某个特定的值，那么用相同的训练集建树得到的结果就会是一模一样的，可以理解为抽取特征的随机种子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec9609",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>知识回顾4</h4><p>\n",
    "    决策树如何处理连续变量和缺失变量？\n",
    "</p></div>\n",
    "\n",
    "对连续变量，决策树一般可以通过随机分割或最佳分割的方式，将连续变量转换成离散变量进行处理。\n",
    "\n",
    "对缺失变量，可以利用修正的信息增益，根据缺失样本比例进行对应的惩罚。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948fa817",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>知识回顾5</h4><p>\n",
    "    基尼系数是什么？为什么要在CART中引入它？\n",
    "</p></div>\n",
    "\n",
    "基尼系数的定义为$E_Y[1-p(Y)]$，起到与信息熵类似的作用，可以衡量混乱程度。\n",
    "\n",
    "在CART中引入它的原因是利用Taylor展开简化比较复杂的对数运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec74cd0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h4>知识回顾6</h4><p>\n",
    "    什么是树的预剪枝和后剪枝？具体分别是如何操作的？\n",
    "</p></div>\n",
    "\n",
    "具体可见上文整理部分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
